{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Add the parent directory to the Python path to load funtions from file ML_funtions\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "# Import helperfunctions\n",
    "from ML_functions import fun_load_data, fun_preprocessing, fun_fit_tuning, fun_load_best_params\n",
    "from ML_functions import fun_convert_time\n",
    "from ML_functions import fun_scaled_neg_MAPE, fun_tuning_results, fun_scores\n",
    "\n",
    "# Assign string \"TSP\" or \"CVRP\" to the following variable to define the optimization problem\n",
    "optimization_problem = \"BPP\"\n",
    "\n",
    "# Load data\n",
    "data = fun_load_data(optimization_problem)\n",
    "\n",
    "# Do the train test split during the preprocessing\n",
    "X_train, X_test, y_train, y_test, train_data = fun_preprocessing(data, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute train score with all features: Neural Network - Multi Layer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpregressor__alpha': 0.5,\n",
       " 'mlpregressor__batch_size': 'auto',\n",
       " 'mlpregressor__hidden_layer_sizes': (100, 100),\n",
       " 'mlpregressor__solver': 'lbfgs'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAPE (scaled) train data:  4.843999999999999 %\n",
      "CV RMSE (scaled) train data: 0.0213\n",
      "CV computation time: 28s\n"
     ]
    }
   ],
   "source": [
    "# Load best parameters of the model\n",
    "best_params = fun_load_best_params(optimization_problem, model_abbreviation=\"NN\")\n",
    "\n",
    "# The solver \"lbfgs\" performed best, but was very slow. Therefore, these tests are done with the best parameters for the \"adam\" solver.\n",
    "best_params = {\"mlpregressor__alpha\": 0.05, \"mlpregressor__batch_size\": \"auto\", \n",
    "               \"mlpregressor__hidden_layer_sizes\": (100, 100), \"mlpregressor__solver\": \"adam\"} # Or \"mlpregressor__hidden_layer_sizes\": (100)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = make_pipeline(StandardScaler(), \n",
    "                     MLPRegressor(activation=\"relu\", learning_rate=\"adaptive\", \n",
    "                                  max_iter=1000, shuffle=True, random_state=0))\n",
    "pipe.set_params(**best_params)\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "model_results_dict_all = fun_scores(pipe, X_train, y_train)\n",
    "\n",
    "#model_results_dict_all = {\"MAPE\": 4.8439, \"RMSE\": 0.0213}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude feature categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Instance ID', 'Number Items', 'Item ID', 'Item Weight Ratio',\n",
       "       'Item Size Ratio', 'Bin Weight', 'Bin Size', 'Weight Size Sum Ratio',\n",
       "       'Item Volume Ratio', 'Item Density Ratio',\n",
       "       'Item Bin Utilization Weight Ratio', 'Item Bin Utilization Size Ratio',\n",
       "       'Item Total Bin Utilization Ratio', 'Weight Bin Combinations Ratio',\n",
       "       'Size Bin Combinations Ratio', 'Total Bin Combinations Ratio',\n",
       "       'Perfect Weight Bin Combinations Ratio',\n",
       "       'Perfect Size Bin Combinations Ratio',\n",
       "       'Perfect Total Bin Combinations Ratio', 'Weight Quantile Values Ratio',\n",
       "       'Size Quantile Values Ratio', '25% Percentile Weight',\n",
       "       '50% Percentile Weight', '75% Percentile Weight', '25% Percentile Size',\n",
       "       '50% Percentile Size', '75% Percentile Size', 'Weight / 0% Percentile',\n",
       "       'Weight / 25% Percentile Ratio', 'Weight / 50% Percentile Ratio',\n",
       "       'Weight / 75% Percentile Ratio', 'Weight / 100% Percentile',\n",
       "       'Size / 0% Percentile', 'Size / 25% Percentile Ratio',\n",
       "       'Size / 50% Percentile Ratio', 'Size / 75% Percentile Ratio',\n",
       "       'Size / 100% Percentile Ratio', 'Weight Sum', 'Size Sum', 'Weight Mean',\n",
       "       'Weight Std', 'Size Std', 'Weight Max', 'Size Max', 'Weight Min',\n",
       "       'Size Min', 'Correlation', 'Skewness Weight', 'Skewness Size',\n",
       "       'Final Bin Utilization Weight', 'Final Bin Utilization Size',\n",
       "       'Final Total Bin Utilization', 'Marginal Cost/Bins Ratio', 'Total Bins',\n",
       "       'Shapley Value'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View all features\n",
    "display(train_data.columns)\n",
    "\n",
    "# Essential feature categories\n",
    "instance_features = [\"Instance ID\", \"Number Items\", \"Item ID\", \"Item Weight Ratio\", \"Item Size Ratio\", \"Bin Weight\", \"Bin Size\"]\n",
    "weight_and_sum_features = [\"Weight Size Sum Ratio\", \"Item Volume Ratio\", \"Item Density Ratio\"]\n",
    "cost_features = [\"Marginal Cost/Bins Ratio\", \"Total Bins\"] #\"Shapley Value\"\n",
    "statistical_features = [\"Weight Sum\", \"Size Sum\", \"Weight Std\", \"Size Std\", \"Weight Max\", \n",
    "                        \"Size Max\", \"Weight Min\", \"Size Min\", \"Correlation\", \"Skewness Weight\", \"Skewness Size\"]\n",
    "\n",
    "# Potential feature categories\n",
    "item_utilization_features = [\"Item Bin Utilization Weight Ratio\", \"Item Bin Utilization Size Ratio\", \"Item Total Bin Utilization Ratio\"]\n",
    "combination_features = [\"Weight Bin Combinations Ratio\", \"Size Bin Combinations Ratio\", \"Total Bin Combinations Ratio\"]\n",
    "perfect_combination_features = [\"Perfect Weight Bin Combinations Ratio\", \"Perfect Size Bin Combinations Ratio\", \"Perfect Total Bin Combinations Ratio\"]\n",
    "quantile_features = [\"Weight Quantile Values Ratio\", \"Size Quantile Values Ratio\"]\n",
    "percentile_features = [\"25% Percentile Weight\", \"50% Percentile Weight\", \"75% Percentile Weight\", \n",
    "                       \"25% Percentile Size\", \"50% Percentile Size\", \"75% Percentile Size\",\n",
    "                       \"Weight / 25% Percentile Ratio\", \"Weight / 50% Percentile Ratio\", \"Weight / 75% Percentile Ratio\", \n",
    "                       \"Size / 25% Percentile Ratio\", \"Size / 50% Percentile Ratio\", \"Size / 75% Percentile Ratio\"]\n",
    "final_utilization_features = [\"Final Bin Utilization Weight\", \"Final Bin Utilization Size\", \"Final Total Bin Utilization\"]\n",
    "\n",
    "# Combine lists to one complete list and one dictionary\n",
    "essential_features = instance_features + weight_and_sum_features + statistical_features + cost_features\n",
    "all_features = essential_features + item_utilization_features + combination_features + perfect_combination_features + quantile_features + percentile_features + final_utilization_features\n",
    "feature_categories_dict = {\"item_utilization_features\": item_utilization_features,\n",
    "                           \"combination_features\": combination_features,\n",
    "                           \"perfect_combination_features\": perfect_combination_features,\n",
    "                           \"quantile_features\": quantile_features,\n",
    "                           \"percentile_features\": percentile_features,\n",
    "                           \"final_utilization_features\": final_utilization_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Excluded feature category: item_utilization_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 46\n",
      "\n",
      "CV MAPE (scaled) train data:  4.9765 %\n",
      "CV RMSE (scaled) train data: 0.0217\n",
      "CV computation time: 24s\n",
      "\n",
      "MAPE difference: 4.9765 - 4.843999999999999 = 0.13250000000000028 %\n",
      "RMSE difference: 0.0217 - 0.0213 = 0.00040000000000000105\n",
      "\n",
      "############### Excluded feature category: combination_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 46\n",
      "\n",
      "CV MAPE (scaled) train data:  5.5963 %\n",
      "CV RMSE (scaled) train data: 0.0243\n",
      "CV computation time: 25s\n",
      "\n",
      "MAPE difference: 5.5963 - 4.843999999999999 = 0.7523000000000009 %\n",
      "RMSE difference: 0.0243 - 0.0213 = 0.002999999999999999\n",
      "\n",
      "############### Excluded feature category: perfect_combination_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 46\n",
      "\n",
      "CV MAPE (scaled) train data:  5.0516 %\n",
      "CV RMSE (scaled) train data: 0.022\n",
      "CV computation time: 28s\n",
      "\n",
      "MAPE difference: 5.0516 - 4.843999999999999 = 0.20760000000000023 %\n",
      "RMSE difference: 0.022 - 0.0213 = 0.0006999999999999992\n",
      "\n",
      "############### Excluded feature category: quantile_features ###############\n",
      "Number of excluded features: 2\n",
      "Number of used features: 47\n",
      "\n",
      "CV MAPE (scaled) train data:  4.825200000000001 %\n",
      "CV RMSE (scaled) train data: 0.0214\n",
      "CV computation time: 24s\n",
      "\n",
      "MAPE difference: 4.825200000000001 - 4.843999999999999 = -0.018799999999998818 %\n",
      "RMSE difference: 0.0214 - 0.0213 = 9.99999999999994e-05\n",
      "\n",
      "############### Excluded feature category: percentile_features ###############\n",
      "Number of excluded features: 12\n",
      "Number of used features: 37\n",
      "\n",
      "CV MAPE (scaled) train data:  5.0074 %\n",
      "CV RMSE (scaled) train data: 0.0219\n",
      "CV computation time: 20s\n",
      "\n",
      "MAPE difference: 5.0074 - 4.843999999999999 = 0.1634000000000002 %\n",
      "RMSE difference: 0.0219 - 0.0213 = 0.0005999999999999998\n",
      "\n",
      "############### Excluded feature category: final_utilization_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 46\n",
      "\n",
      "CV MAPE (scaled) train data:  5.3644 %\n",
      "CV RMSE (scaled) train data: 0.0231\n",
      "CV computation time: 23s\n",
      "\n",
      "MAPE difference: 5.3644 - 4.843999999999999 = 0.5204000000000004 %\n",
      "RMSE difference: 0.0231 - 0.0213 = 0.0017999999999999995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in feature_categories_dict.keys():\n",
    "    print(\"############### Excluded feature category: {} ###############\".format(key))\n",
    "    \n",
    "    # Select only the used features in the train set\n",
    "    excluded_features = [i for i in feature_categories_dict[key]] + [i + \" Ratio\" for i in feature_categories_dict[key]]\n",
    "    used_features = [i for i in all_features if i not in excluded_features]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print(\"Number of excluded features:\", len(feature_categories_dict[key]))\n",
    "    print(\"Number of used features: {}\\n\".format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of all categories\n",
    "    MAPE_diff = model_results_dict_new[\"MAPE\"] - model_results_dict_all[\"MAPE\"]\n",
    "    RMSE_diff = model_results_dict_new[\"RMSE\"] - model_results_dict_all[\"RMSE\"]\n",
    "    print(\"\\nMAPE difference: {} - {} = {} %\".format(model_results_dict_new[\"MAPE\"], model_results_dict_all[\"MAPE\"], MAPE_diff))\n",
    "    print(\"RMSE difference: {} - {} = {}\\n\".format(model_results_dict_new[\"RMSE\"], model_results_dict_all[\"RMSE\"], RMSE_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add single features to the essential features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of essential features: 27\n"
     ]
    }
   ],
   "source": [
    "# Essential feature categories\n",
    "instance_features = [\"Instance ID\", \"Number Items\", \"Item ID\", \"Item Weight Ratio\", \"Item Size Ratio\", \"Bin Weight\", \"Bin Size\"]\n",
    "weight_and_sum_features = [\"Weight Size Sum Ratio\", \"Item Volume Ratio\", \"Item Density Ratio\"]\n",
    "statistical_features = [\"Weight Sum\", \"Size Sum\", \"Weight Std\", \"Size Std\", \"Weight Max\", \n",
    "                        \"Size Max\", \"Weight Min\", \"Size Min\", \"Correlation\", \"Skewness Weight\", \"Skewness Size\"]\n",
    "cost_features = [\"Marginal Cost/Bins Ratio\", \"Total Bins\"] #\"Shapley Value\"\n",
    "total_features = [\"Item Total Bin Utilization Ratio\", \"Total Bin Combinations Ratio\", \"Perfect Total Bin Combinations Ratio\", \"Final Total Bin Utilization\"]\n",
    "\n",
    "# Potential feature categories\n",
    "item_utilization_features = [\"Item Bin Utilization Weight Ratio\", \"Item Bin Utilization Size Ratio\"]\n",
    "combination_features = [\"Weight Bin Combinations Ratio\", \"Size Bin Combinations Ratio\"]\n",
    "perfect_combination_features = [\"Perfect Weight Bin Combinations Ratio\", \"Perfect Size Bin Combinations Ratio\"]\n",
    "quantile_features = [\"Weight Quantile Values Ratio\", \"Size Quantile Values Ratio\"]\n",
    "percentile_features1 = [\"25% Percentile Weight\", \"50% Percentile Weight\", \"75% Percentile Weight\", \n",
    "                       \"25% Percentile Size\", \"50% Percentile Size\", \"75% Percentile Size\"]\n",
    "percentile_features2 = [\"Weight / 25% Percentile Ratio\", \"Weight / 50% Percentile Ratio\", \"Weight / 75% Percentile Ratio\", \n",
    "                       \"Size / 25% Percentile Ratio\", \"Size / 50% Percentile Ratio\", \"Size / 75% Percentile Ratio\"]\n",
    "final_utilization_features = [\"Final Bin Utilization Weight\", \"Final Bin Utilization Size\"]\n",
    "\n",
    "# Combine lists to one complete list and one dictionary\n",
    "essential_features = instance_features + weight_and_sum_features + statistical_features + cost_features + total_features\n",
    "all_features = essential_features + item_utilization_features + combination_features + perfect_combination_features + quantile_features + percentile_features1 + percentile_features2 + final_utilization_features\n",
    "feature_categories_dict = {\"item_utilization_features\": item_utilization_features,\n",
    "                           \"combination_features\": combination_features,\n",
    "                           \"perfect_combination_features\": perfect_combination_features,\n",
    "                           \"quantile_features\": quantile_features,\n",
    "                           \"percentile_features1\": percentile_features1,\n",
    "                           \"percentile_features2\": percentile_features2,\n",
    "                           \"final_utilization_features\": final_utilization_features}\n",
    "\n",
    "print(\"Number of essential features:\", len(essential_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAPE (scaled) train data:  5.115 %\n",
      "CV RMSE (scaled) train data: 0.0229\n",
      "CV computation time: 22s\n"
     ]
    }
   ],
   "source": [
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "X_train_small = X_train[essential_features]\n",
    "model_results_dict_essential = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "#model_results_dict_essential = {\"MAPE\": 5.115, \"RMSE\": 0.0229}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Added features: ###############\n",
      " ['Item Bin Utilization Weight Ratio', 'Item Bin Utilization Size Ratio']\n",
      "Number of used features: 29\n",
      "\n",
      "CV MAPE (scaled) train data:  5.2522 %\n",
      "CV RMSE (scaled) train data: 0.0232\n",
      "CV computation time: 26s\n",
      "\n",
      "MAPE difference: 5.2522 - 5.115 = 0.1372 %\n",
      "RMSE difference: 0.0232 - 0.0229 = 0.0002999999999999982\n",
      "\n",
      "############### Added features: ###############\n",
      " ['Weight Bin Combinations Ratio', 'Size Bin Combinations Ratio']\n",
      "Number of used features: 29\n",
      "\n",
      "CV MAPE (scaled) train data:  5.196 %\n",
      "CV RMSE (scaled) train data: 0.0229\n",
      "CV computation time: 25s\n",
      "\n",
      "MAPE difference: 5.196 - 5.115 = 0.08099999999999952 %\n",
      "RMSE difference: 0.0229 - 0.0229 = 0.0\n",
      "\n",
      "############### Added features: ###############\n",
      " ['Perfect Weight Bin Combinations Ratio', 'Perfect Size Bin Combinations Ratio']\n",
      "Number of used features: 29\n",
      "\n",
      "CV MAPE (scaled) train data:  5.1047 %\n",
      "CV RMSE (scaled) train data: 0.0225\n",
      "CV computation time: 21s\n",
      "\n",
      "MAPE difference: 5.1047 - 5.115 = -0.010299999999999976 %\n",
      "RMSE difference: 0.0225 - 0.0229 = -0.00040000000000000105\n",
      "\n",
      "############### Added features: ###############\n",
      " ['Weight Quantile Values Ratio', 'Size Quantile Values Ratio']\n",
      "Number of used features: 29\n",
      "\n",
      "CV MAPE (scaled) train data:  5.2619 %\n",
      "CV RMSE (scaled) train data: 0.023\n",
      "CV computation time: 20s\n",
      "\n",
      "MAPE difference: 5.2619 - 5.115 = 0.1468999999999996 %\n",
      "RMSE difference: 0.023 - 0.0229 = 9.99999999999994e-05\n",
      "\n",
      "############### Added features: ###############\n",
      " ['25% Percentile Weight', '50% Percentile Weight', '75% Percentile Weight', '25% Percentile Size', '50% Percentile Size', '75% Percentile Size']\n",
      "Number of used features: 33\n",
      "\n",
      "CV MAPE (scaled) train data:  5.2688 %\n",
      "CV RMSE (scaled) train data: 0.023\n",
      "CV computation time: 20s\n",
      "\n",
      "MAPE difference: 5.2688 - 5.115 = 0.1537999999999995 %\n",
      "RMSE difference: 0.023 - 0.0229 = 9.99999999999994e-05\n",
      "\n",
      "############### Added features: ###############\n",
      " ['Weight / 25% Percentile Ratio', 'Weight / 50% Percentile Ratio', 'Weight / 75% Percentile Ratio', 'Size / 25% Percentile Ratio', 'Size / 50% Percentile Ratio', 'Size / 75% Percentile Ratio']\n",
      "Number of used features: 33\n",
      "\n",
      "CV MAPE (scaled) train data:  5.284599999999999 %\n",
      "CV RMSE (scaled) train data: 0.0232\n",
      "CV computation time: 19s\n",
      "\n",
      "MAPE difference: 5.284599999999999 - 5.115 = 0.16959999999999908 %\n",
      "RMSE difference: 0.0232 - 0.0229 = 0.0002999999999999982\n",
      "\n",
      "############### Added features: ###############\n",
      " ['Final Bin Utilization Weight', 'Final Bin Utilization Size']\n",
      "Number of used features: 29\n",
      "\n",
      "CV MAPE (scaled) train data:  4.9963 %\n",
      "CV RMSE (scaled) train data: 0.0224\n",
      "CV computation time: 19s\n",
      "\n",
      "MAPE difference: 4.9963 - 5.115 = -0.11870000000000047 %\n",
      "RMSE difference: 0.0224 - 0.0229 = -0.0005000000000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "potential_features = [item_utilization_features, combination_features, perfect_combination_features, quantile_features, percentile_features1, percentile_features2, final_utilization_features]\n",
    "\n",
    "# Add iteratively a single feature or a list of features to the essential features and compute the score difference (compared to the score of the essential features only)\n",
    "for added_features in potential_features:\n",
    "    print(\"############### Added features: ###############\\n {}\".format(added_features))\n",
    "\n",
    "    # Select only the used features in the train set\n",
    "    if isinstance(added_features, list): # Check whether added_feature is a single feature (string) or a list of features\n",
    "        used_features = essential_features + added_features\n",
    "    else: used_features = essential_features + [added_features]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print(\"Number of used features: {}\\n\".format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of the essential features only\n",
    "    MAPE_diff = model_results_dict_new[\"MAPE\"] - model_results_dict_essential[\"MAPE\"]\n",
    "    RMSE_diff = model_results_dict_new[\"RMSE\"] - model_results_dict_essential[\"RMSE\"]\n",
    "    print(\"\\nMAPE difference: {} - {} = {} %\".format(model_results_dict_new[\"MAPE\"], model_results_dict_essential[\"MAPE\"], MAPE_diff))\n",
    "    print(\"RMSE difference: {} - {} = {}\\n\".format(model_results_dict_new[\"RMSE\"], model_results_dict_essential[\"RMSE\"], RMSE_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update essential features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAPE (scaled) train data:  4.9702 %\n",
      "CV RMSE (scaled) train data: 0.022\n",
      "CV computation time: 19s\n"
     ]
    }
   ],
   "source": [
    "essential_features += perfect_combination_features + final_utilization_features\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "X_train_small = X_train[essential_features]\n",
    "model_results_dict_essential = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "#model_results_dict_essential = {\"MAPE\": 4.9702, \"RMSE\": 0.022}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used features: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Instance ID',\n",
       " 'Number Items',\n",
       " 'Item ID',\n",
       " 'Item Weight Ratio',\n",
       " 'Item Size Ratio',\n",
       " 'Bin Weight',\n",
       " 'Bin Size',\n",
       " 'Weight Size Sum Ratio',\n",
       " 'Item Volume Ratio',\n",
       " 'Item Density Ratio',\n",
       " 'Weight Sum',\n",
       " 'Size Sum',\n",
       " 'Weight Std',\n",
       " 'Size Std',\n",
       " 'Weight Max',\n",
       " 'Size Max',\n",
       " 'Weight Min',\n",
       " 'Size Min',\n",
       " 'Correlation',\n",
       " 'Skewness Weight',\n",
       " 'Skewness Size',\n",
       " 'Marginal Costs/Bins Ratio',\n",
       " 'Total Bins',\n",
       " 'Item Total Bin Utilization Ratio',\n",
       " 'Total Bin Combinations Ratio',\n",
       " 'Perfect Total Bin Combinations Ratio',\n",
       " 'Final Total Bin Utilization',\n",
       " 'Perfect Weight Bin Combinations Ratio',\n",
       " 'Perfect Size Bin Combinations Ratio',\n",
       " 'Final Bin Utilization Weight',\n",
       " 'Final Bin Utilization Size']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of used features:\", len(essential_features))\n",
    "display(essential_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe not necessary features\n",
    "[\"Perfect Weight Bin Combinations Ratio\",\n",
    " \"Perfect Size Bin Combinations Ratio\",\n",
    " \"Final Bin Utilization Weight\",\n",
    " \"Final Bin Utilization Size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item Bin Utilization Weight Ratio',\n",
       " 'Item Bin Utilization Size Ratio',\n",
       " 'Weight Bin Combinations Ratio',\n",
       " 'Size Bin Combinations Ratio',\n",
       " 'Weight Quantile Values Ratio',\n",
       " 'Size Quantile Values Ratio',\n",
       " '25% Percentile Weight',\n",
       " '50% Percentile Weight',\n",
       " '75% Percentile Weight',\n",
       " '25% Percentile Size',\n",
       " '50% Percentile Size',\n",
       " '75% Percentile Size',\n",
       " 'Weight / 25% Percentile Ratio',\n",
       " 'Weight / 50% Percentile Ratio',\n",
       " 'Weight / 75% Percentile Ratio',\n",
       " 'Size / 25% Percentile Ratio',\n",
       " 'Size / 50% Percentile Ratio',\n",
       " 'Size / 75% Percentile Ratio']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropped_features = item_utilization_features + combination_features + quantile_features + percentile_features1 + percentile_features2\n",
    "display(dropped_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
