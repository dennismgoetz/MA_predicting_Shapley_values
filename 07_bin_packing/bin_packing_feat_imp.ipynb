{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Add the parent directory to the Python path to load funtions from file ML_funtions\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "# Import helperfunctions\n",
    "from ML_functions import fun_load_data, fun_preprocessing, fun_fit_tuning, fun_load_best_params\n",
    "from ML_functions import fun_convert_time\n",
    "from ML_functions import fun_scaled_neg_MAPE, fun_tuning_results, fun_scores\n",
    "\n",
    "# Assign string 'TSP' or 'CVRP' to the following variable to define the optimization problem\n",
    "optimization_problem = 'Bin_Packing'\n",
    "train_size = 0.7\n",
    "\n",
    "# Load data\n",
    "data = fun_load_data(optimization_problem)\n",
    "X, y, train_data = fun_preprocessing(data)\n",
    "\n",
    "# Create a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute train score with all features: Neural Network - Multi Layer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpregressor__alpha': 0.5,\n",
       " 'mlpregressor__batch_size': 'auto',\n",
       " 'mlpregressor__hidden_layer_sizes': (100, 100),\n",
       " 'mlpregressor__solver': 'lbfgs'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAPE (scaled) train data:  4.8044 %\n",
      "CV RMSE (scaled) train data: 0.0213\n",
      "CV computation time: 25s\n"
     ]
    }
   ],
   "source": [
    "# Load best parameters of the model\n",
    "best_params = fun_load_best_params(optimization_problem + '_NN_GS_best_params.pkl')\n",
    "\n",
    "# best_params = {'mlpregressor__alpha': 0.5, 'mlpregressor__batch_size': 'auto', \n",
    "#                'mlpregressor__hidden_layer_sizes': (100, 100), 'mlpregressor__solver': 'lbfgs'}\n",
    "\n",
    "# The solver 'lbfgs' performed best, but was very slow. Therefore, these tests are done with the best parameters for the 'adam' solver.\n",
    "best_params = {'mlpregressor__alpha': 0.05, 'mlpregressor__batch_size': 'auto', \n",
    "               'mlpregressor__hidden_layer_sizes': (100, 100), 'mlpregressor__solver': 'adam'} # Or 'mlpregressor__hidden_layer_sizes': (100)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = make_pipeline(StandardScaler(), \n",
    "                     MLPRegressor(activation='relu', learning_rate='adaptive', \n",
    "                                  max_iter=1000, shuffle=True, random_state=0))\n",
    "pipe.set_params(**best_params)\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "model_results_dict_all = fun_scores(pipe, X_train, y_train)\n",
    "\n",
    "#model_results_dict_all = {'MAPE': 4.8044, 'RMSE': 0.0213}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude feature categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Instance ID', 'Number Items', 'Item ID', 'Item Weight Ratio',\n",
       "       'Item Size Ratio', 'Bin Weight', 'Bin Size', 'Weight Size Sum Ratio',\n",
       "       'Item Volume Ratio', 'Item Density Ratio',\n",
       "       'Item Bin Utilization Weight Ratio', 'Item Bin Utilization Size Ratio',\n",
       "       'Item Total Bin Utilization Ratio', 'Weight Bin Combinations Ratio',\n",
       "       'Size Bin Combinations Ratio', 'Total Bin Combinations Ratio',\n",
       "       'Perfect Weight Bin Combinations Ratio',\n",
       "       'Perfect Size Bin Combinations Ratio',\n",
       "       'Perfect Total Bin Combinations Ratio', 'Weight Quantile Values Ratio',\n",
       "       'Size Quantile Values Ratio', '0% Percentile Weight',\n",
       "       '25% Percentile Weight', '50% Percentile Weight',\n",
       "       '75% Percentile Weight', '100% Percentile Weight', '0% Percentile Size',\n",
       "       '25% Percentile Size', '50% Percentile Size', '75% Percentile Size',\n",
       "       '100% Percentile Size', 'Weight / 0% Percentile Ratio',\n",
       "       'Weight / 25% Percentile Ratio', 'Weight / 50% Percentile Ratio',\n",
       "       'Weight / 75% Percentile Ratio', 'Weight / 100% Percentile Ratio',\n",
       "       'Size / 0% Percentile Ratio', 'Size / 25% Percentile Ratio',\n",
       "       'Size / 50% Percentile Ratio', 'Size / 75% Percentile Ratio',\n",
       "       'Size / 100% Percentile Ratio', 'Weight Sum', 'Size Sum', 'Weight Mean',\n",
       "       'Size Mean', 'Weight Std', 'Size Std', 'Weight Max', 'Size Max',\n",
       "       'Weight Min', 'Size Min', 'Correlation', 'Skewness Weight',\n",
       "       'Skewness Size', 'Final Bin Utilization Weight',\n",
       "       'Final Bin Utilization Size', 'Final Total Bin Utilization',\n",
       "       'Marginal Costs/Bins Ratio', 'Total Bins', 'Shapley Value'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View all features\n",
    "display(train_data.columns)\n",
    "\n",
    "instance_features = ['Instance ID', 'Number Items', 'Item ID', 'Item Weight Ratio', 'Item Size Ratio', 'Bin Weight', 'Bin Size']\n",
    "weight_and_sum_features = ['Weight Size Sum Ratio', 'Item Volume Ratio', 'Item Density Ratio']\n",
    "item_utilization_features = ['Item Bin Utilization Weight Ratio', 'Item Bin Utilization Size Ratio', 'Item Total Bin Utilization Ratio']\n",
    "combination_features = ['Weight Bin Combinations Ratio', 'Size Bin Combinations Ratio', 'Total Bin Combinations Ratio',\n",
    "                        'Perfect Weight Bin Combinations Ratio', 'Perfect Size Bin Combinations Ratio', 'Perfect Total Bin Combinations Ratio']\n",
    "quantile_features = ['Weight Quantile Values Ratio', 'Size Quantile Values Ratio']\n",
    "percentile_features = ['0% Percentile Weight', '25% Percentile Weight', '50% Percentile Weight', '75% Percentile Weight', '100% Percentile Weight', \n",
    "                       '0% Percentile Size', '25% Percentile Size', '50% Percentile Size', '75% Percentile Size', '100% Percentile Size',\n",
    "                       'Weight / 0% Percentile Ratio', 'Weight / 25% Percentile Ratio', 'Weight / 50% Percentile Ratio', 'Weight / 75% Percentile Ratio', 'Weight / 100% Percentile Ratio', \n",
    "                       'Size / 0% Percentile Ratio', 'Size / 25% Percentile Ratio', 'Size / 50% Percentile Ratio', 'Size / 75% Percentile Ratio', 'Size / 100% Percentile Ratio']\n",
    "statistical_features = ['Weight Sum', 'Size Sum', 'Weight Mean', 'Size Mean', 'Weight Std', 'Size Std', 'Weight Max', \n",
    "                        'Size Max', 'Weight Min', 'Size Min', 'Correlation', 'Skewness Weight', 'Skewness Size']\n",
    "final_utilization_features = ['Final Bin Utilization Weight', 'Final Bin Utilization Size', 'Final Total Bin Utilization']\n",
    "cost_features = ['Marginal Costs/Bins Ratio', 'Total Bins'] #'Shapley Value'\n",
    "\n",
    "# Combine lists to one complete list and one dictionary\n",
    "all_features = instance_features + weight_and_sum_features + item_utilization_features + combination_features + quantile_features + percentile_features + statistical_features + final_utilization_features + cost_features\n",
    "feature_categories_dict = {'weight_and_sum_features': weight_and_sum_features,\n",
    "                           'item_utilization_features': item_utilization_features,\n",
    "                           'combination_features':combination_features,\n",
    "                           'quantile_features': quantile_features,\n",
    "                           'percentile_features': percentile_features,\n",
    "                           'statistical_features': statistical_features,\n",
    "                           'final_utilization_features': final_utilization_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Excluded feature category: weight_and_sum_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 56\n",
      "\n",
      "CV MAPE (scaled) train data:  4.8663 %\n",
      "CV RMSE (scaled) train data: 0.0214\n",
      "CV computation time: 33s\n",
      "\n",
      "MAPE difference: 4.8663 - 4.8044 = 0.06189999999999962 %\n",
      "RMSE difference: 0.0214 - 0.0213 = 9.99999999999994e-05\n",
      "\n",
      "############### Excluded feature category: item_utilization_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 56\n",
      "\n",
      "CV MAPE (scaled) train data:  4.8587 %\n",
      "CV RMSE (scaled) train data: 0.0214\n",
      "CV computation time: 30s\n",
      "\n",
      "MAPE difference: 4.8587 - 4.8044 = 0.05429999999999957 %\n",
      "RMSE difference: 0.0214 - 0.0213 = 9.99999999999994e-05\n",
      "\n",
      "############### Excluded feature category: combination_features ###############\n",
      "Number of excluded features: 6\n",
      "Number of used features: 53\n",
      "\n",
      "CV MAPE (scaled) train data:  5.5044 %\n",
      "CV RMSE (scaled) train data: 0.0244\n",
      "CV computation time: 25s\n",
      "\n",
      "MAPE difference: 5.5044 - 4.8044 = 0.7000000000000002 %\n",
      "RMSE difference: 0.0244 - 0.0213 = 0.003100000000000002\n",
      "\n",
      "############### Excluded feature category: quantile_features ###############\n",
      "Number of excluded features: 2\n",
      "Number of used features: 57\n",
      "\n",
      "CV MAPE (scaled) train data:  4.9399 %\n",
      "CV RMSE (scaled) train data: 0.0218\n",
      "CV computation time: 22s\n",
      "\n",
      "MAPE difference: 4.9399 - 4.8044 = 0.1354999999999995 %\n",
      "RMSE difference: 0.0218 - 0.0213 = 0.0005000000000000004\n",
      "\n",
      "############### Excluded feature category: percentile_features ###############\n",
      "Number of excluded features: 20\n",
      "Number of used features: 39\n",
      "\n",
      "CV MAPE (scaled) train data:  4.7343 %\n",
      "CV RMSE (scaled) train data: 0.0211\n",
      "CV computation time: 22s\n",
      "\n",
      "MAPE difference: 4.7343 - 4.8044 = -0.07010000000000005 %\n",
      "RMSE difference: 0.0211 - 0.0213 = -0.0001999999999999988\n",
      "\n",
      "############### Excluded feature category: statistical_features ###############\n",
      "Number of excluded features: 13\n",
      "Number of used features: 46\n",
      "\n",
      "CV MAPE (scaled) train data:  5.1202 %\n",
      "CV RMSE (scaled) train data: 0.0222\n",
      "CV computation time: 22s\n",
      "\n",
      "MAPE difference: 5.1202 - 4.8044 = 0.3157999999999994 %\n",
      "RMSE difference: 0.0222 - 0.0213 = 0.0009000000000000015\n",
      "\n",
      "############### Excluded feature category: utilization_features ###############\n",
      "Number of excluded features: 3\n",
      "Number of used features: 56\n",
      "\n",
      "CV MAPE (scaled) train data:  5.1961 %\n",
      "CV RMSE (scaled) train data: 0.0225\n",
      "CV computation time: 25s\n",
      "\n",
      "MAPE difference: 5.1961 - 4.8044 = 0.39170000000000016 %\n",
      "RMSE difference: 0.0225 - 0.0213 = 0.0011999999999999997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in feature_categories_dict.keys():\n",
    "    print('############### Excluded feature category: {} ###############'.format(key))\n",
    "    \n",
    "    # Select only the used features in the train set\n",
    "    excluded_features = [i for i in feature_categories_dict[key]] + [i + ' Ratio' for i in feature_categories_dict[key]]\n",
    "    used_features = [i for i in all_features if i not in excluded_features]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print('Number of excluded features:', len(feature_categories_dict[key]))\n",
    "    print('Number of used features: {}\\n'.format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of all categories\n",
    "    MAPE_diff = model_results_dict_new['MAPE'] - model_results_dict_all['MAPE']\n",
    "    RMSE_diff = model_results_dict_new['RMSE'] - model_results_dict_all['RMSE']\n",
    "    print('\\nMAPE difference: {} - {} = {} %'.format(model_results_dict_new['MAPE'], model_results_dict_all['MAPE'], MAPE_diff))\n",
    "    print('RMSE difference: {} - {} = {}\\n'.format(model_results_dict_new['RMSE'], model_results_dict_all['RMSE'], RMSE_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add single features to the essential features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_features = ['Depot Distance Ratio', 'Closest Customer Distance (CCD) Ratio', '2nd CCD Ratio', '3rd CCD Ratio', '4th CCD Ratio', '5th CCD Ratio', \n",
    "                     '6th CCD Ratio', '7th CCD Ratio', '8th CCD Ratio', 'Mean Distance To Other Customers Ratio', 'Gravity Center Distance Ratio']\n",
    "essential_features = instance_features + distance_features + statistical_features + cost_features\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "X_train_small = X_train[essential_features]\n",
    "model_results_dict_essential = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "#model_results_dict_essential = {'MAPE': 2.9232, 'RMSE': 0.7813}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_features = ['Number Clusters', 'Number Outliers', 'Cluster Size', 'Cluster', ['X Centroid', 'Y Centroid'], \n",
    "                      'Centroid Distance Ratio', 'Centroid Distance To Depot Ratio', 'Distance To Closest Other Cluster Ratio', \n",
    "                      'Distance To Closest Other Centroid Ratio', 'Cluster Area Ratio', 'Cluster Density Ratio']\n",
    "\n",
    "# Add iteratively a single feature or a list of features to the essential features and compute the score difference (compared to the score of the essential features only)\n",
    "for added_feature in potential_features:\n",
    "    print('############### Added feature: {} ###############'.format(added_feature))\n",
    "\n",
    "    # Select only the used features in the train set\n",
    "    if isinstance(added_feature, list): # Check whether added_feature is a single feature (string) or a list of features\n",
    "        used_features = all_features + added_feature\n",
    "    else: used_features = essential_features + [added_feature]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print('Number of used features: {}\\n'.format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of the essential features only\n",
    "    MAPE_diff = model_results_dict_new['MAPE'] - model_results_dict_essential['MAPE']\n",
    "    RMSE_diff = model_results_dict_new['RMSE'] - model_results_dict_essential['RMSE']\n",
    "    print('\\nMAPE difference: {} - {} = {} %'.format(model_results_dict_new['MAPE'], model_results_dict_essential['MAPE'], MAPE_diff))\n",
    "    print('RMSE difference: {} - {} = {}\\n'.format(model_results_dict_new['RMSE'], model_results_dict_essential['RMSE'], RMSE_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude single features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_features = ['Depot Distance Ratio', 'Closest Customer Distance (CCD) Ratio', '2nd CCD Ratio', '3rd CCD Ratio', '4th CCD Ratio', '5th CCD Ratio', \n",
    "                     '6th CCD Ratio', '7th CCD Ratio', '8th CCD Ratio', 'Mean Distance To Other Customers Ratio', 'Gravity Center Distance Ratio']\n",
    "all_features = instance_features + distance_features + cluster_features + statistical_features + cost_features\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "X_train_small = X_train[all_features]\n",
    "#model_results_dict_all = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "model_results_dict_all = {'MAPE': 2.8593, 'RMSE': 0.7692}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_features = ['Cluster', 'Number Clusters', 'Number Outliers', 'Cluster Size', ['X Centroid', 'Y Centroid'], \n",
    "                      'Centroid Distance Ratio', 'Centroid Distance To Depot Ratio', 'Distance To Closest Other Cluster Ratio', \n",
    "                      'Distance To Closest Other Centroid Ratio', 'Cluster Area Ratio', 'Cluster Density Ratio']\n",
    "\n",
    "# Add iteratively a single feature or a list of features to the essential features and compute the score difference (compared to the score of the essential features only)\n",
    "for excluded_feature in potential_features:\n",
    "    print('############### Excluded feature: {} ###############'.format(excluded_feature))\n",
    "\n",
    "    # Select only the used features in the train set\n",
    "    if isinstance(excluded_feature, list): # Check whether added_feature is a single feature (string) or a list of features\n",
    "        used_features = [i for i in all_features if i not in excluded_feature]\n",
    "    else: used_features = [i for i in all_features if i != excluded_feature]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print('Number of used features: {}\\n'.format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of the essential features only\n",
    "    MAPE_diff = model_results_dict_new['MAPE'] - model_results_dict_all['MAPE']\n",
    "    RMSE_diff = model_results_dict_new['RMSE'] - model_results_dict_all['RMSE']\n",
    "    print('\\nMAPE difference: {} - {} = {} %'.format(model_results_dict_new['MAPE'], model_results_dict_all['MAPE'], MAPE_diff))\n",
    "    print('RMSE difference: {} - {} = {}\\n'.format(model_results_dict_new['RMSE'], model_results_dict_all['RMSE'], RMSE_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclude updated feature categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features categories with their features\n",
    "distance_features = ['Depot Distance Ratio', 'Closest Customer Distance (CCD) Ratio', '2nd CCD Ratio', '3rd CCD Ratio', '4th CCD Ratio', '5th CCD Ratio', \n",
    "                     '6th CCD Ratio', '7th CCD Ratio', '8th CCD Ratio', 'Mean Distance To Other Customers Ratio', 'Gravity Center Distance Ratio']\n",
    "cluster_features = ['Cluster', 'Number Clusters', 'Cluster Size', 'X Centroid', 'Y Centroid', 'Centroid Distance To Depot Ratio',\n",
    "                    'Distance To Closest Other Cluster Ratio', 'Cluster Area Ratio', 'Cluster Density Ratio']\n",
    "statistical_features = ['X Std', 'Y Std', 'X Max', 'Y Max', 'X Min', 'Y Min', 'Correlation', 'Skewness X', 'Skewness Y']\n",
    "\n",
    "# Combine lists to one complete list and one dictionary\n",
    "all_features = instance_features + distance_features + cluster_features + statistical_features + cost_features\n",
    "feature_categories_dict = {'distance_features': distance_features,\n",
    "                           'cluster_features': cluster_features,\n",
    "                           'statistical_features': statistical_features}\n",
    "\n",
    "X_train_small = X_train[all_features]\n",
    "\n",
    "# Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "model_results_dict_all = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "#model_results_dict_all = {'MAPE': 2.8642, 'RMSE': 0.7637}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in feature_categories_dict.keys():\n",
    "    #if (key == 'cluster_features'):\n",
    "    print('############### Excluded feature category: {} ###############'.format(key))\n",
    "    \n",
    "    # Select only the used features in the train set\n",
    "    excluded_features = [i for i in feature_categories_dict[key]] + [i + ' Ratio' for i in feature_categories_dict[key]]\n",
    "    used_features = [i for i in all_features if i not in excluded_features]\n",
    "    X_train_small = X_train[used_features]\n",
    "    print('Number of excluded features:', len(feature_categories_dict[key]))\n",
    "    print('Number of used features: {}\\n'.format(len(used_features)))\n",
    "    #display(used_features)\n",
    "\n",
    "    # Estimate model performance with cross-validation on the train set (scoring: MAPE and RMSE)\n",
    "    model_results_dict_new = fun_scores(pipe, X_train_small, y_train)\n",
    "\n",
    "    # Compare the new results with the results of all categories\n",
    "    MAPE_diff = model_results_dict_new['MAPE'] - model_results_dict_all['MAPE']\n",
    "    RMSE_diff = model_results_dict_new['RMSE'] - model_results_dict_all['RMSE']\n",
    "    print('\\nMAPE difference: {} - {} = {} %'.format(model_results_dict_new['MAPE'], model_results_dict_all['MAPE'], MAPE_diff))\n",
    "    print('RMSE difference: {} - {} = {}\\n'.format(model_results_dict_new['RMSE'], model_results_dict_all['RMSE'], RMSE_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
